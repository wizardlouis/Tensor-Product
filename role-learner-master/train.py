from __future__ import unicode_literals, print_function, division
from role_assignment_functions import *
from rolelearner.role_learning_tensor_product_encoder import RoleLearningTensorProductEncoder
from tasks import *
from training import *
from models import *
from evaluation import *
from io import open
import unicodedata
import string
import re
import random
from random import shuffle

import torch
import torch.nn as nn
from torch.autograd import Variable
from torch import optim
import torch.nn.functional as F

import sys
import os

import time
import math

import pickle
import argparse

from sklearn import metrics
import numpy as np

use_cuda = torch.cuda.is_available()

if use_cuda:
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

parser = argparse.ArgumentParser()
parser.add_argument("--data_prefix", help="prefix for the vectors", type=str, default=None)
parser.add_argument("--role_prefix", help="prefix for a file of roles (if used)", type=str,
                    default=None)
parser.add_argument("--role_scheme", help="pre-coded role scheme to use", type=str, default=None)
parser.add_argument("--test_decoder", help="whether to test the decoder (in addition to MSE",
                    type=str, default="False")
parser.add_argument("--decoder", help="decoder type", type=str, default="ltr")
parser.add_argument("--decoder_prefix", help="prefix for the decoder to test", type=str,
                    default=None)
parser.add_argument("--decoder_embedding_size", help="embedding size for decoder", type=int,
                    default=20)
parser.add_argument("--decoder_task", help="task performed by the decoder", type=str,
                    default="auto")
parser.add_argument("--filler_dim", help="embedding dimension for fillers", type=int, default=10)
parser.add_argument("--role_dim", help="embedding dimension for roles", type=int, default=6)
parser.add_argument("--vocab_size", help="vocab size for the training language", type=int,
                    default=10)
parser.add_argument("--hidden_size", help="size of the encodings", type=int, default=60)
parser.add_argument("--save_vectors",
                    help="whether to save vectors generated by the fitted TPR model", type=str,
                    default="False")
parser.add_argument("--save_role_dicts",
                    help="whether to save role_to_index and index_to_role or not", type=str,
                    default="False")
parser.add_argument("--embedding_file", help="file containing pretrained embeddings", type=str,
                    default=None)
parser.add_argument("--unseen_words",
                    help="if using pretrained embeddings: whether to use all zeroes for unseen "
						 "words' embeddings, or to give them random vectors",
                    type=str, default="zero")
parser.add_argument("--extra_test_set", help="additional file to print predictions for", type=str,
                    default=None)
parser.add_argument("--train", help="whether or not to train the model", type=str, default="True")
parser.add_argument("--neighbor_analysis", help="whether to use a neighbor analysis", type=str,
                    default="True")
parser.add_argument("--digits", help="whether this is one of the digit task", type=str,
                    default="True")
parser.add_argument("--final_linear", help="whether to have a final linear layer", type=str,
                    default="True")
parser.add_argument("--embed_squeeze", help="original dimension to be squeezed to filler_dim",
                    type=int, default=None)
parser.add_argument("--role_learning", help="A flag for whether to enable role learning or use "
                                            "the provided roles.", action="store_true")
parser.add_argument("--bidirectional", help="A flag for whether the role learning module is "
                                            "bidirectional.", action="store_true")
parser.add_argument(
    "--role_assignment_shrink_filler_dim",
    help="If specified, the filler embedding is shrunk to this size before being input to the "
         "role assignment LSTM.",
    default=None,
    type=int)
parser.add_argument(
    "--use_one_hot_temperature",
    help="A flag for whether role learning one hot regularization should have an increasing "
         "temperature.",
    action="store_true")
parser.add_argument(
    "--role_assigner_num_layers",
    help="The number of layers for the role assignment network.",
    default=1,
    type=int
)
parser.add_argument(
    "--output_dir",
    help="An optional output folder where files can be saved to.",
    type=str,
    default=None
)
parser.add_argument(
    "--patience",
    help="The number of epochs to train if validation loss isn't improving",
    type=int,
    default=10
)
parser.add_argument(
    "--pretrained_filler_embedding",
    help="A weight file containing a pretrained filler embedding",
    type=str,
    default=None
)
parser.add_argument(
    "--softmax_roles",
    help="Whether the role predictions should be run through a softmax",
    action="store_true"
)
parser.add_argument(
    "--batch_size",
    help="The batch size.",
    default=32,
    type=int
)
parser.add_argument(
    "--one_hot_regularization_weight",
    help="The weight applied to the one hot regularization term",
    type=float,
    default=1.0
)
parser.add_argument(
    "--l2_norm_regularization_weight",
    help="The weight applied to the l2 norm regularization term",
    type=float,
    default=1.0
)
parser.add_argument(
    "--unique_role_regularization_weight",
    help="The weight applied to the unique role regularization term",
    type=float,
    default=1.0
)
parser.add_argument(
    "--num_roles",
    help="The number of roles to give a role learning network. This value is only used when "
         "--role_learning is enabled.",
    type=int,
    default=None
)
parser.add_argument(
    "--data_path",
    help="The location of the data files.",
    type=str,
    default="data"
)
parser.add_argument(
    "--burn_in",
    help="The number of epochs to train without regularization",
    type=int,
    default=0
)
parser.add_argument(
    "--shuffle",
    help="Whether to shuffle the input fillers and corresponding embedding. This is to generate a"
         " value by which to normalize the MSE to compare different role schemes across "
         "different tasks.",
    action="store_true"
)
parser.add_argument(
    "--scan_checkpoint",
    help="The location of a SCAN checkpoint. Settings this argument enables the SCAN task.",
    type=str,
    default=None,
    required=False
)


args = parser.parse_args()

output_dir = None
if args.output_dir:
    output_dir = os.path.join('output/', args.output_dir)
    if not os.path.isdir(output_dir):
        os.makedirs(output_dir)
    with open(os.path.join(output_dir, 'arguments.txt'), 'w') as arguments_file:
        for argument, value in sorted(vars(args).items()):
            arguments_file.write('{}: {}\n'.format(argument, value))

# Create the logfiled
if output_dir:
    results_page = open(os.path.join(output_dir, 'log.txt'), 'w')
else:
    if args.final_linear != "True":
        results_page = open(
            "logs/" + args.data_prefix.split("/")[-1] + str(args.role_prefix).split("/")[-1] + str(
                args.role_scheme) + ".filler" + str(args.filler_dim) + ".role" + str(
                args.role_dim) + ".tpr_decomp.nf", "w")
    else:
        results_page = open(
            "logs/" + args.data_prefix.split("/")[-1] + str(args.role_prefix).split("/")[-1] + str(
                args.role_scheme) + ".filler" + str(args.filler_dim) + ".role" + str(
                args.role_dim) + "." + str(args.embed_squeeze) + ".tpr_decomp", "w")

# Prepare the train, dev, and test data
unindexed_train = []
unindexed_dev = []
unindexed_test = []
unindexed_extra = []

filler_to_index = {}
index_to_filler = {}
role_to_index = {}
index_to_role = {}

filler_counter = 0
role_counter = 0
max_length = 0

train_file = open(os.path.join(args.data_path, args.data_prefix + ".data_from_train"), "r")
for line in train_file:
    sequence, vector = line.strip().split("\t")
    unindexed_train.append(([value for value in sequence.split()], Variable(
        torch.FloatTensor(np.array([float(value) for value in vector.split()])))))

    if len(sequence.split()) > max_length:
        max_length = len(sequence.split())

    for filler in sequence.split():
        if filler not in filler_to_index:
            filler_to_index[filler] = filler_counter
            index_to_filler[filler_counter] = filler
            filler_counter += 1

dev_file = open(os.path.join(args.data_path, args.data_prefix + ".data_from_dev"), "r")
for line in dev_file:
    sequence, vector = line.strip().split("\t")
    unindexed_dev.append(([value for value in sequence.split()], Variable(
        torch.FloatTensor(np.array([float(value) for value in vector.split()])))))

    if len(sequence.split()) > max_length:
        max_length = len(sequence.split())

    for filler in sequence.split():
        if filler not in filler_to_index:
            filler_to_index[filler] = filler_counter
            index_to_filler[filler_counter] = filler
            filler_counter += 1

test_file = open(os.path.join(args.data_path, args.data_prefix + ".data_from_test"), "r")
for line in test_file:
    sequence, vector = line.strip().split("\t")
    unindexed_test.append(([value for value in sequence.split()], Variable(
        torch.FloatTensor(np.array([float(value) for value in vector.split()])))))

    if len(sequence.split()) > max_length:
        max_length = len(sequence.split())

    for filler in sequence.split():
        if filler not in filler_to_index:
            filler_to_index[filler] = filler_counter
            index_to_filler[filler_counter] = filler
            filler_counter += 1

# Initialize the TPDN

if args.final_linear == "True":
    final_layer_width = args.hidden_size
else:
    final_layer_width = None

if args.role_learning:
    if args.num_roles:
        role_counter = args.num_roles
    print("Using RoleLearningTensorProductEncoder with {} roles".format(role_counter))
    tpr_encoder = RoleLearningTensorProductEncoder(
        n_roles=role_counter,
        n_fillers=filler_counter,
        final_layer_width=final_layer_width,
        filler_dim=args.filler_dim,
        role_dim=args.role_dim,
        pretrained_filler_embeddings=args.pretrained_filler_embedding,
        embedder_squeeze=args.embed_squeeze,
        role_assignment_shrink_filler_dim=args.role_assignment_shrink_filler_dim,
        bidirectional=args.bidirectional,
        num_layers=args.role_assigner_num_layers,
        softmax_roles=args.softmax_roles,
        pretrained_embeddings=None,
        one_hot_regularization_weight=args.one_hot_regularization_weight,
        l2_norm_regularization_weight=args.l2_norm_regularization_weight,
        unique_role_regularization_weight=args.unique_role_regularization_weight,
    )

if use_cuda:
    tpr_encoder = tpr_encoder.cuda()

#train the encoder
args.role_prefix = str(args.role_prefix).split("/")[-1]
if args.train == "True":
    if output_dir:
        weight_file = os.path.join(output_dir, 'model.tpr')
    else:
        weight_file = "models/" + args.data_prefix + str(
                                  args.role_prefix) + str(args.role_scheme) + ".tpr"
    end_loss = trainIters_tpr(
        all_train_data,
        all_dev_data,
        tpr_encoder,
        n_epochs=1000,
        learning_rate=0.001,
        weight_file=weight_file,
        batch_size=args.batch_size,
        use_one_hot_temperature=args.use_one_hot_temperature,
        patience=args.patience,
        burn_in=args.burn_in
    )
print("Finished training")
